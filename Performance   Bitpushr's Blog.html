<!DOCTYPE html>
<!-- saved from url=(0052)https://bitpushr.wordpress.com/category/performance/ -->
<html lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta charset="UTF-8">
	<title>Performance | Bitpushr's Blog</title>

	<link rel="pingback" href="https://bitpushr.wordpress.com/xmlrpc.php">
	<!--[if lt IE 9]>
		<script src="https://s0.wp.com/wp-content/themes/pub/manifest/js/html5.js" type="text/javascript"></script>
	<![endif]-->

	<link rel="alternate" type="application/rss+xml" title="Bitpushr&#39;s Blog » Feed" href="https://bitpushr.wordpress.com/feed/">
<link rel="alternate" type="application/rss+xml" title="Bitpushr&#39;s Blog » Comments Feed" href="https://bitpushr.wordpress.com/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="Bitpushr&#39;s Blog » Performance Category Feed" href="https://bitpushr.wordpress.com/category/performance/feed/">
<script type="text/javascript">
/* <![CDATA[ */
function addLoadEvent(func){var oldonload=window.onload;if(typeof window.onload!='function'){window.onload=func;}else{window.onload=function(){oldonload();func();}}}
/* ]]> */
</script>
<link rel="stylesheet" id="all-css-0" href="./Performance   Bitpushr's Blog_files/saved_resource" type="text/css" media="all">
<!--[if IE 7]>
<link rel='stylesheet' id='manifest-ie-css'  href='https://s0.wp.com/wp-content/themes/pub/manifest/css/style_ie.css?ver=4.1-alpha-20141011' type='text/css' media='all' />
<![endif]-->
<link rel="stylesheet" id="all-css-2" href="./Performance   Bitpushr's Blog_files/saved_resource(1)" type="text/css" media="all">
<link rel="stylesheet" id="print-css-2" href="./Performance   Bitpushr's Blog_files/global-print.css" type="text/css" media="print">
<script type="text/javascript">
/* <![CDATA[ */
var LoggedOutFollow = {"invalid_email":"Your subscription did not succeed, please try again with a valid email address."};
/* ]]> */
</script>
<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/saved_resource(2)"></script><style type="text/css"></style>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://bitpushr.wordpress.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s1.wp.com/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress.com">
<link rel="shortcut icon" type="image/x-icon" href="https://s2.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48">
<link rel="icon" type="image/x-icon" href="https://s2.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48">
<link rel="apple-touch-icon-precomposed" href="https://s0.wp.com/i/webclip.png">
<link rel="openid.server" href="https://bitpushr.wordpress.com/?openidserver=1">
<link rel="openid.delegate" href="https://bitpushr.wordpress.com/">
<link rel="search" type="application/opensearchdescription+xml" href="https://bitpushr.wordpress.com/osd.xml" title="Bitpushr&#39;s Blog">
<link rel="search" type="application/opensearchdescription+xml" href="http://wordpress.com/opensearch.xml" title="WordPress.com">
<meta name="application-name" content="Bitpushr&#39;s Blog"><meta name="msapplication-window" content="width=device-width;height=device-height"><meta name="msapplication-tooltip" content="I push bits around."><meta name="msapplication-task" content="name=Subscribe;action-uri=https://bitpushr.wordpress.com/feed/;icon-uri=https://s2.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s2.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s2.wp.com/i/favicon.ico"><meta name="title" content="Posts about Performance on Bitpushr&#39;s Blog">
<meta name="description" content="Posts about Performance written by bitpushr">
<style type="text/css" id="syntaxhighlighteranchor"></style>
<link rel="stylesheet" type="text/css" id="gravatar-card-css" href="./Performance   Bitpushr's Blog_files/hovercard.css"><link rel="stylesheet" type="text/css" id="gravatar-card-services-css" href="./Performance   Bitpushr's Blog_files/services.css"><script type="text/javascript" async="" src="./Performance   Bitpushr's Blog_files/saved_resource(3)"></script></head>

<body class="archive category category-performance category-1930 mp6 highlander-enabled highlander-light infinite-scroll neverending" data-twttr-rendered="true">

<div id="site-wrapper">
	<h1 class="vcard author" id="site-title"><a href="https://bitpushr.wordpress.com/" title="Home" class="fn" sl-processed="1">Bitpushr's Blog</a></h1>
	<nav id="main-nav">
			<ul>
		<li class="page_item page-item-2"><a href="https://bitpushr.wordpress.com/about/" sl-processed="1">About</a></li>
	</ul>
	</nav>
		<div id="site-description">
		I push bits around.	</div>

		<h2 class="archive-title">Category: <strong>Performance</strong></h2>

	
	<div id="core-content" class="hfeed">

	
				
			
<div class="post-606 post type-post status-publish format-standard hentry category-performance category-storage" id="post-606">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2014/10/14/characterizing-workloads-in-data-ontap/" sl-processed="1">October 14, 2014</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2014/10/14/characterizing-workloads-in-data-ontap/" rel="bookmark" sl-processed="1">Characterizing workloads in Data&nbsp;ONTAP</a></h3>		
		<div class="entry-content">
			<p>One of the most important things when analyzing Data ONTAP performance is being able to characterize the workload that you’re seeing. This can be done a couple of ways, but I’ll outline one here. Note that this is not a primer on analyzing storage bottlenecks — for that, you’ll have to attend Insight this year! — but rather a way to see how much data a system is serving out and what kind of data it is.</p>
<p>When it comes to characterizing workloads on a system, everyone has a different approach — and that’s totally fine. I tend to start with a few basic questions:</p>
<ol>
<li>Where is the data being served from?</li>
<li>How much is being served?</li>
<li>What protocols are doing the most work?</li>
<li>What kind of work are the protocols doing?</li>
</ol>
<p>You can do this with the the <tt>stats</tt> command if you know what counters to look for. If you find a statistic and you don’t know what it means, we have a good help system at hand. To wit and to use <tt>stats explain counters</tt>:</p>
<pre># rsh charles "priv set -q diag; stats explain counters readahead:readahead:incore"
Counters for object name: readahead
Name: incore
Description: Total number of blocks that were already resident
Properties: delta
Unit: none</pre>
<p>Okay, so that’s not the greatest explanation, but it’s a start! More commonly-used counters are going to have better descriptions.</p>
<p>When it comes to analyzing your workload, a good place to start are the <tt>read_io_type</tt> counters in the &lt;ttwafl object. These statistics are not normally collected, so you have to start counting and stop counting them. Here’s an example on a 7-mode system — I’m going to use rsh non-interactively because there are a <b>lot</b> of counters!</p>
<pre># rsh charles "priv set -q diag; stats show wafl:wafl:read_io_type"
wafl:wafl:<span class="skimlinks-unlinked">read_io_type.cache:0</span>%
wafl:wafl:read_io_type.ext_cache:0%
wafl:wafl:<span class="skimlinks-unlinked">read_io_type.disk:0</span>%
wafl:wafl:read_io_type.bamboo_ssd:0%
wafl:wafl:read_io_type.hya_hdd:0%
wafl:wafl:read_io_type.hya_cache:0%</pre>
<p>Here is an explanation of these counters:</p>
<ul>
<li><tt>cache</tt> are reads served from the Filer’s RAM</li>
<li><tt>ext_cache</tt> are reads served from the Filer’s Flash Cache</li>
<li><tt>disk</tt> are reads served from any model of spinning disk</li>
<li><tt>bamboo_ssd</tt> are reads served from any model of solid-state disk</li>
<li><tt>hya_hdd</tt> are reads served from the spinning disks that are part of a Flash Pool</li>
<li><tt>hya_cache</tt> are reads served from the solid-state disks that are part of a Flash Pool</li>
</ul>
<p>Note that these counters are present regardless of your system’s configuration. For example, the Flash Cache counters will be visible even if you don’t have Flash Cache installed.</p>
<p>Now that you know where your reads are being served from, you might be curious as to how big the reads are, and if they’re random or sequential. Those statistics are available, but it takes a bit more grepping and cutting to get them. Here’s a little from my volume, <tt>sefiles</tt>. Note that I need to prefix this capture with <tt>priv set</tt>:</p>
<pre># rsh charles "priv set -q diag; stats start -I perfstat_nfs"
# rsh charles "priv set -q diag; stats stop -I perfstat_nfs"</pre>
<p>Okay, so there’s a <em>lot</em> of output there. It’s data for the entire system. That said, it’s a great way to learn — you can see everything the system sees. And if you dump it to a text file, it will save you using <tt>rsh</tt> over and over again! Let’s use <tt>rsh</tt> again and drill down to the <tt>volume</tt> stats, which is the most useful way to investigate a particular volume:</p>
<pre># rsh charles "priv set -q diag; stats show volume:sefiles"
volume:sefiles:instance_name:sefiles
volume:sefiles:instance_uuid:a244800b-5506-11e1-91cc-123478563412
volume:sefiles:parent_aggr:aggr1
volume:sefiles:avg_latency:425.77us
volume:sefiles:total_ops:93/s
volume:sefiles:read_data:6040224b/s
volume:sefiles:read_latency:433.44us
volume:sefiles:read_ops:92/s
volume:sefiles:write_data:0b/s
volume:sefiles:write_latency:0us
volume:sefiles:write_ops:0/s
volume:sefiles:other_latency:12.17us
volume:sefiles:other_ops:1/s
volume:sefiles:nfs_read_data:6040224b/s
volume:sefiles:nfs_read_latency:433.44us
volume:sefiles:nfs_read_ops:92/s
volume:sefiles:nfs_write_data:0b/s
volume:sefiles:nfs_write_latency:0us
volume:sefiles:nfs_write_ops:0/s
volume:sefiles:nfs_other_latency:15.09us
volume:sefiles:nfs_other_ops:0/s</pre>
<p>I’ve removed some of the internal counters for clarity. Now, let’s take a look at a couple of things: the volume name is listed as <tt>sefiles</tt> in the <tt>instance_name</tt> variable. During the time I was capturing statistics, the average amount of operations per second was 93 (“<tt>total_ops</tt>“). Read latency (“<tt>read_latency</tt>“) was 433us, which is 0.433ms. Notice that we did no writes (<tt>"write_ops"</tt>) and basically no metadata either (<tt>"other_ops"</tt>).</p>
<p>If you’re interested in seeing what the system is doing in general, I tend to go protocol-by-protocol. If you grep for <tt>nfsv3</tt>, you can start to wade through things:</p>
<pre># rsh charles "priv set -q diag; stats show nfsv3:nfs:nfsv3_read_latency_hist"
nfsv3:nfs:nfsv3_read_latency_hist.0 - &lt;1ms:472
nfsv3:nfs:nfsv3_read_latency_hist.1 - &lt;2ms:7
nfsv3:nfs:nfsv3_read_latency_hist.2 - &lt;4ms:10
nfsv3:nfs:nfsv3_read_latency_hist.4 - &lt;6ms:8
nfsv3:nfs:nfsv3_read_latency_hist.6 - &lt;8ms:7
nfsv3:nfs:nfsv3_read_latency_hist.8 - &lt;10ms:0
nfsv3:nfs:nfsv3_read_latency_hist.10 - &lt;12ms:1</pre>
<p>During our statistics capture, approximately 472 requests were served at 1ms or better. 7 requests were served between 2ms and 1ms. 10 requests were served between 4ms and 2ms, and so on. If you keep looking, you’ll see write latencies as well:</p>
<pre># rsh charles "priv set -q diag; stats show nfsv3:nfs:nfsv3_write_latency_hist"
nfsv3:nfs:nfsv3_write_latency_hist.0 - &lt;1ms:1761
nfsv3:nfs:nfsv3_write_latency_hist.1 - &lt;2ms:0
nfsv3:nfs:nfsv3_write_latency_hist.2 - &lt;4ms:2
nfsv3:nfs:nfsv3_write_latency_hist.4 - &lt;6ms:1
nfsv3:nfs:nfsv3_write_latency_hist.6 - &lt;8ms:0
nfsv3:nfs:nfsv3_write_latency_hist.8 - &lt;10ms:0
nfsv3:nfs:nfsv3_write_latency_hist.10 - &lt;12ms:0</pre>
<p>So that’s 1761 writes at 1ms or better. Your writes should always show excellent latencies as writes are acknowledged once they’re committed to NVRAM — the actual writing of the data to disk will occur at the next consistency point.</p>
<p>Keep going and you’ll see latency figures that are concatenated for all types of requests — reads, writes, other (i.e. metadata):</p>
<pre># rsh charles "priv set -q diag; stats show nfsv3:nfs:nfsv3_latency_hist"
nfsv3:nfs:nfsv3_latency_hist.0 - &lt;1ms:2124
nfsv3:nfs:nfsv3_latency_hist.1 - &lt;2ms:5
nfsv3:nfs:nfsv3_latency_hist.2 - &lt;4ms:16
nfsv3:nfs:nfsv3_latency_hist.4 - &lt;6ms:18
nfsv3:nfs:nfsv3_latency_hist.6 - &lt;8ms:13
nfsv3:nfs:nfsv3_latency_hist.8 - &lt;10ms:6
nfsv3:nfs:nfsv3_latency_hist.10 - &lt;12ms:1</pre>
<p>Once you’ve established latencies, you may be interested to see the I/O sizes of the actual requests. Behold:</p>
<pre># rsh charles "priv set -q diag; stats show nfsv3:nfs:nfsv3_read_size_histo"
nfsv3:nfs:nfsv3_read_size_histo.0-511:0
nfsv3:nfs:nfsv3_read_size_histo.512-1023:0
nfsv3:nfs:nfsv3_read_size_histo.1K-2047:0
nfsv3:nfs:nfsv3_read_size_histo.2K-4095:0
nfsv3:nfs:nfsv3_read_size_histo.4K-8191:328
nfsv3:nfs:nfsv3_read_size_histo.8K-16383:71</pre>
<p>These statistics are represented like the latencies were: 0 requests were served that were between 0-511 bytes in size, 0 between 512 &amp; 1023 bytes, and so on; then, 328 requests were served that were between 4KB &amp; 8KB in size and 71 requests between 8KB &amp; 16KB.</p>
<p>When it comes to protocol operations, we break things down by the unique operation functions — including metadata operations. I’ll show the count numbers, but it’ll also show percentages of requests and individual latencies for each:</p>
<pre># rsh charles "priv set -q diag; stats show nfsv3:nfs:nfsv3_op_count"
nfsv3:nfs:nfsv3_op_count.getattr:4
nfsv3:nfs:nfsv3_op_count.setattr:0
nfsv3:nfs:nfsv3_op_count.lookup:0
nfsv3:nfs:nfsv3_op_count.access:1
nfsv3:nfs:nfsv3_op_count.readlink:0
nfsv3:nfs:nfsv3_op_count.read:424
nfsv3:nfs:nfsv3_op_count.write:1767
nfsv3:nfs:nfsv3_op_count.create:0
nfsv3:nfs:nfsv3_op_count.mkdir:0
nfsv3:nfs:nfsv3_op_count.symlink:0
nfsv3:nfs:nfsv3_op_count.mknod:0
nfsv3:nfs:nfsv3_op_count.remove:0
nfsv3:nfs:nfsv3_op_count.rmdir:0
nfsv3:nfs:nfsv3_op_count.rename:0
nfsv3:nfs:nfsv3_op_count.link:0
nfsv3:nfs:nfsv3_op_count.readdir:0
nfsv3:nfs:nfsv3_op_count.readdirplus:0</pre>
<p>In this case, most of my requests were reads &amp; writes; there was very little metadata operation (just 4 attribute lookups). If you want to know what these operations represent, you can read some of my previous blog entries or check out the RFC that defines your protocol. </p>
<p>One last thing that I forgot to add is trying to figure out if your workload is random or sequential. The easiest way that I’ve found to do that is to look in the <tt>readahead</tt> stats, as the readahead engine is a particularly useful way of determining how your reads are operating. We don’t need to focus on writes as much because any write much larger than 16KB is coalesced in memory and written sequentially — even if it started as random at the client. (I’ll talk about RAVE, the new readahead engine in DOT 8.1+, in later posts.)</p>
<p>I’ve filtered out some of the irrelevant stuff. So let’s gather what we can use and see:</p>
<pre># rsh charles "priv set -q diag; stats show readahead:readahead:total_read_reqs"
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.4K:629</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.8K:77</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.12K:0</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.16K:4</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.20K:1</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.24K:7</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.28K:8</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.32K:0</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.40K:0</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.48K:0</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.56K:0</span>
readahead:readahead:<span class="skimlinks-unlinked">total_read_reqs.64K:77662</span></pre>
<p>These statistics work much the same as our previous ones. 629 read requests were serviced that were 4KB blocks, 77 were serviced that were 8KB blocks, etc. The numbers continue right up to 1024KB. (Keep this in mind whenever someone tells you that WAFL &amp; ONTAP can only read or write 4KB at a time!) Now if you want to see sequentiality:</p>
<pre># rsh charles "priv set -q diag; stats show readahead:readahead:seq_read_reqs"
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.4K:36</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.8K:46</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.12K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.16K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.20K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.24K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.28K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.32K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.40K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.48K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.56K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">seq_read_reqs.64K:96</span>%</pre>
<p>We can see that, of the 4KB reads that we serviced, 36% were sequential. Mathematically this means that ~64% would be random, which we can confirm here:</p>
<pre># rsh charles "priv set -q diag; stats show readahead:readahead:rand_read_reqs"
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.4K:63</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.8K:53</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.12K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.16K:100</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.20K:100</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.24K:100</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.28K:100</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.32K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.40K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.48K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.56K:0</span>%
readahead:readahead:<span class="skimlinks-unlinked">rand_read_reqs.64K:3</span>%</pre>
<p>Here, of our 4KB requests, 63% of them are random. Of our 8KB requests, 53% were random. Notice that the random percentages and sequential percentages for each block histogram will add up to ~100% depending on rounding. If you dig deeper into the stats, readahead can provide you with some useful information about how your data came to be read:</p>
<pre># rsh charles "priv set -q diag; stats readahead:readahead"
readahead:readahead:requested:2217395
readahead:readahead:read:82524
readahead:readahead:incore:25518
readahead:readahead:speculative:79658
readahead:readahead:read_once:82200</pre>
<p>You can interpret those numbers thusly. Again, I’ve removed the irrelevant stuff:</p>
<ul>
<li><tt>requested</tt> is the sum of all blocks requested by the client</li>
<li><tt>read</tt> are the blocks actually read from disk (where the readahead engine had a “miss”)</li>
<li><tt>incore</tt> are the blocks read from cache (where the readahead engine had a “hit”)</li>
<li><tt>speculative</tt> are blocks that were readahead because we figured the client might need them later in the I/O stream</li>
<li><tt>read_once</tt> are blocks that were not read ahead and was not cached because it didn’t match the cache policy (e.g. <tt> flexscale.enable.lopri_blocks</tt> being off)</li>
</ul>
<p>With this information, you should have a pretty good idea about how to characterize what your Filer is doing for workloads. Although I’ve focused on NFSv3 here, the same techniques can be applied to the other protocols. If you’ve got any questions, please holler!</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2014/10/14/characterizing-workloads-in-data-ontap/#respond" title="Comment on Characterizing workloads in Data ONTAP" sl-processed="1">Leave a comment</a>				</div>
	</div>
</div>
		
			
<div class="post-396 post type-post status-publish format-standard hentry category-performance category-storage" id="post-396">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2014/07/28/how-data-ontap-caches-assembles-and-writes-data/" sl-processed="1">July 28, 2014</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2014/07/28/how-data-ontap-caches-assembles-and-writes-data/" rel="bookmark" sl-processed="1">How Data ONTAP caches, assembles and writes&nbsp;data</a></h3>		
		<div class="entry-content">
			<p>In this post, I thought I would try and describe how ONTAP works <em>and why</em>. I’ll also explain what the “anywhere” means in the <strong>Write Anywhere File Layout</strong>, a.k.a. WAFL. </p>
<p>A couple of common points of confusion are the role that NVRAM plays in performance, and where our write optimization comes from. I’ll attempt to cover both of these here. I’ll also try and cover how we handle parity. Before we start, here are some terms (and ideas) that matter. This isn’t NetApp 101, but rather NetApp 100.5 — I’m not going to cover all the basics, but I’ll cover the basics that are relevant here. So here goes!</p>
<h1>What hardware is in a controller, anyway?</h1>
<p>NetApp storage controllers <em>contain</em> some essential ingredients: hardware and software. In terms of hardware the controller has CPUs, RAM and NVRAM. In terms of software the controller has its operating system, Data ONTAP. Our CPUs do what everyone else’s CPUs do — they run our operating system. Our RAM does what everyone else’s RAM does — it runs our operating system. Our RAM also has a very important function in that it serves as a cache. While our NVRAM does <em>roughly</em> what everyone else’s NVRAM does — i.e., it contains our transaction journal — a key difference is the <em>way</em> we use NVRAM.</p>
<p>That said, at NetApp we do things in unique ways. And although we have one operating system, Data ONTAP, we have several hardware platforms. They range from small controllers to big controllers, which medium controllers in between. Different controllers have different amounts of CPU, RAM &amp; NVRAM but the principles are the same!</p>
<h2>CPU</h2>
<p>Our CPUs run the Data ONTAP operating system, and they also <em>process</em> data for clients. Our controllers vary from a single dual-core CPU (in a FAS2220) to a pair of hex-core CPUs (in a FAS6290). The higher the amount of client I/O, the harder the CPUs work. Not all protocols are equal, though; serving 10 IOps via CIFS generates a different CPU load than serving 10 IOps via FCP.</p>
<h2>RAM</h2>
<p>Our RAM contains the Data ONTAP operating system, and it also <em>caches</em> data for clients. It is also the source for all writes that are committed to disk via consistency points. Writes do <strong>not</strong> come from NVRAM! Our controllers vary from 6GB of RAM (FAS2220) to 96GB of RAM (FAS6290). Not all workloads are equal, though; different features and functionality requires a different memory footprint.</p>
<h2>NVRAM</h2>
<p>Physically, NVRAM is little more than RAM with a battery backup. Our NVRAM contains a transaction log of client I/O <em>that has not yet been written to disk from RAM by a consistency point</em>. Its primary mission is to preserve that not-yet-written data in the event of a power outage or similar, severe problem. Our controllers vary from 768MB of NVRAM (FAS2220) to 4GB of NVRAM (FAS6290). In my opinion, NVRAM’s function is perhaps the most-commonly misunderstood part of our architecture. NVRAM is simply a double-buffered journal of pending write operations. NVRAM, therefore, is simply a redo log — it is not the write cache! After data is written to NVRAM, it is not looked at again unless you experience a dirty shutdown. This is because NVRAM’s importance to performance comes from software. </p>
<p>In an HA pair environment where two controllers are connected to each other, NVRAM is mirrored between the two nodes. Its primary mission is to preserve data that not-yet-written data in the event a partner controller suffers a power outage or similar severe problem. NVRAM mirroring happens for HA pairs in Data ONTAP 7-mode, HA pairs in clustered Data ONTAP and HA pairs in MetroCluster environments.</p>
<h2>Disks and disk shelves</h2>
<p>Disk shelves contain disks. DS14 shelves contain 14 drives, and DS2246 &amp; DS4243 &amp; DS4246 shelves contain 24 disks. DS4248 shelves contain 48 disks. Disk shelves are connected to controllers via shelf modules, those logical connections run either FCAL (DS14) or SAS (DS2246/DS42xx) protocols for connectivity.</p>
<h1>What software is in a controller, anyway?</h1>
<h2>Data ONTAP</h2>
<p>Data ONTAP is our controller’s operating system. Almost everything sits here — from configuration files and databases to license keys, log files and some diagnostic tools. Our operating system is built on top of FreeBSD, and usually lives in a volume called <tt>vol0</tt>. Data ONTAP features implementations of protocols for client access (e.g. NFS, CIFS), APIs for programming access (ZAPI) and implementations of protocols for management access (SSH). It is fair to say that Data ONTAP is the heart of a NetApp controller.</p>
<h2>WAFL</h2>
<p>WAFL is our Write Anywhere File Layout. If NVRAM’s role is the most-commonly misunderstood, WAFL comes in 2nd. Yet WAFL has a simple goal, which is to write data in full stripes across the storage media. WAFL acts as an intermediary of sorts — there is a <a href="https://communities.netapp.com/community/netapp-blogs/dave/blog/2008/12/08/is-wafl-a-filesystem" sl-processed="1">top half</a> where files and volumes sit, and a <a href="https://communities.netapp.com/community/netapp-blogs/dave/blog/2008/12/08/is-wafl-a-filesystem" sl-processed="1">bottom half</a> that interacts with RAID, manages SnapShots and some other things. WAFL isn’t a filesystem, but it does some things a filesystem does; it can also <em>contain</em> filesystems. </p>
<p>WAFL contains mechanisms for dealing with files &amp; directories, for interacting with volumes &amp; aggregates, and for interacting with RAID. If Data ONTAP is the heart of a NetApp controller, WAFL is the blood that it pumps. </p>
<p>Although WAFL can write anywhere we want, in reality we write where it makes the most sense: in the closest place (relative to the disk head) where we can write a complete stripe in order to minimize seek time on subsequent I/O requests. WAFL is optimized for writes, and we’ll see why below. Rather unusually for storage arrays, we can write client data <em>and</em> metadata anywhere. </p>
<p>A colleague has this to say about WAFL, and I couldn’t put it better:</p>
<blockquote><p>There is a relatively simple “cheating at Tetris” analogy that can be used to articulate WAFL’s advantages. It is not hard to imagine how good you could be at Tetris if you were able to review the next <strong>thousand</strong> shapes that were falling into the pattern, rather than just the next shape. </p>
<p>Now imagine how much better you could be at Tetris if you could take any of the shapes from within the next thousand to place into your pattern, rather than being forced to use just the next shape that is falling. </p>
<p>Finally, imagine having plenty of time to review the next thousand shapes and plan your layout of all 1,000, rather than just a second or two to figure out what to do with the next piece that is falling. In summary, you could become the best Tetris player on Earth, and that is essentially what WAFL is in the arena of data allocation techniques onto underlying disk arrays.</p></blockquote>
<p>The Tetris analogy incredibly important, as it directly relates to the way that NetApp uses WAFL to optimize for writes. Essentially, we collect random I/O that is destined to be written to disk, reorganize it so that it resembles sequential I/O as much as possible, <strong>and then write it to disk sequentially</strong>. Another way of explaining this behavior is that of <strong>write coalescing</strong>: we reduce the number of operations that ultimately land on the disk, <em>because</em> we re-organize them in memory before we commit them to disk <em>and we wait</em> until we have a bunch of them before committing them to disk via a Consistency Point. Put another way, write coalescing allows to avoid the common (and expensive) RAID workflow of “read-modify-write”.</p>
<h1>Putting it all together</h1>
<p>NetApp storage arrays are made up of controllers and disk shelves. Where the top and bottom is depends on your perspective: disks are grouped into RAID groups, and those RAID groups are combined to make aggregates. Volumes live in aggregates, and files and LUNs live in those volumes. A volume of CIFS data is shared to a client via a SMB share; a volume of NFS data is shared to a client via a NFS export. A LUN of data is shared to a client via an FCP, FCOE or ISCSI initiator group. Note the relationship here between controller and client — all clients care about are volumes, files and LUNs. They don’t care <em>directly</em> about CPUs, NVRAM or really anything else when it comes to hardware. There is data and I/O and that’s it.</p>
<p>In order to get data to and from clients as quickly as possible, NetApp engineers have done much to try to optimize controller performance. A lot of the architectural design you see in our controllers reflects this. Although the clients don’t care directly about how NetApp is architected, our architecture does matter to the way their underlying data is handled and the way their I/O is served. Here is a basic workflow, from the inimitable <a href="http://www.recoverymonkey.org/" sl-processed="1">Recovery Monkey</a> himself:</p>
<ol>
<li>The client writes some data</li>
<li>Write arrives in controller’s RAM and is copied into controller’s NVRAM</li>
<li>Write is then mirrored into partner’s NVRAM</li>
<li>The client receives an acknowledgement that the data has been written</li>
</ol>
<p>Sounds pretty simple right? On the surface, it is a pretty simple process. It also explains a few core concepts about NetApp architecture:</p>
<ul>
<li>Because we acknowledge the client write once it’s hit NVRAM, we’re optimized for writes out of the box</li>
<li>Because we don’t need to wait for the disks, we can write anywhere we choose to</li>
<li>Why we can survive an outage to either half of an HA pair</li>
</ul>
<p>The 2nd bullet provides the backbone of this post. Because we can write data anywhere we choose to, we tend to write data in the best place possible. This is typically in the largest contiguous stripe of free space in the volume’s aggregate (closest to the disk heads). After 10 seconds have elapsed, or if NVRAM becomes &gt;=50% full, we write the client’s data from RAM (<em>not</em> from NVRAM) to disk.</p>
<p>This operation is called a <strong>consistency point</strong>. Because we use RAID, a consistency point requires us to perform RAID calculations and to calculate parity. These calculations are processed by the CPUs using data that exists in RAM.</p>
<p>Many people think that our acceleration occurs because of NVRAM. Rather, a lot of our acceleration happens while we’re waiting for NVRAM. For example — and very significantly — we transmogrify random I/O from the client into sequential I/O before writing it to disk. This processing is done by the CPU and occurs in RAM. Another significant benefit is because we calculate data in RAM, we do not need to hammer the disk drives that contain parity information.  </p>
<p>So, with the added step of a CP, this is how things really happen:</p>
<ol>
<li>The client writes some data</li>
<li>Write arrives in controller’s RAM and is copied into controller’s NVRAM</li>
<li>Write is then mirrored into partner’s NVRAM</li>
<li>When the partner NVRAM acknowledges the write, the client receives an acknowledgement that the data has been written [to disk]</li>
<li>A consistency point occurs and the data (incl. parity) is written to disk</li>
</ol>
<p>To re-iterate, writes are cached in the controller’s RAM at the same time as being logged into NVRAM (and the partner’s NVRAM if you’re using a HA pair). Once the data has been written to disk via a CP, the writes are purged from the controller’s NVRAM but retained (with a lower priority) in the controller’s RAM. The prioritization allows us to evict less commonly-used blocks in order to avoid overrunning the system memory. In other words, recently-written data is the first to be ejected from the first-level read cache in the controller’s RAM.</p>
<p>The parity issue also catches people out. Traditional filesystems and arrays write data (and metadata) into pre-allocated locations; Data ONTAP and WAFL let NetApp write data (and metadata) in whatever location will provide fastest access. This is usually a stripe to the nearest available set of free blocks. The ability of WAFL to write to the nearest available free disk blocks lets us <strong>greatly reduce disk seeking</strong>. This is the #1 performance challenge when using spinning disks! It also lets us avoid the “hot parity disk” paradigm, as WAFL always writes to new, free disk blocks using pre-calculated parity.</p>
<p>Consistency points are also commonly misunderstood. The purpose of a consistency point is simple: to write data to free space on disks. Once a CP has taken place, the contents of NVRAM are discarded. Incoming writes that are received during the actual process of a consistency point’s commitment (i.e., the actual writing to disk) will be written to disk in the <em>next</em> consistency point.</p>
<p>The workflow for a write:</p>
<p>1. Write is sent from the host to the storage system (via a NIC or HBA)<br>
2. Write is processed into system memory while a) being logged in NVRAM and b) being logged in the HA partner’s NVRAM<br>
3. Write is acknowledged to the host<br>
4. Write is committed to disk storage in a consistency point (CP)</p>
<p>The importance of consistency points cannot be overstated — they are a cornerstone of Data ONTAP’s architecture! They are also the reason why Data ONTAP is optimized for writes: no matter the destination disk type, we acknowledge client writes as soon as they hit NVRAM, and NVRAM is always faster than any time of disk! </p>
<p>Consistency points typically they occur every 10 seconds or whenever NVRAM begins to get full, whichever comes first. The latter is referred to as a “watermark”. Imagine that NVRAM is a bucket, and now divide that bucket in half. One side of the bucket is for incoming data (from clients) and the other side of the bucket is for outgoing data (to disks). As one side of the bucket fills, the other side drains. In an HA pair, we actually divide NVRAM into four buckets: two for the local controller and two for the partner controller. (This is why, when activating or deactivating HA on a system, a reboot is required for the change to take effect. The 10-second rule is why, on a system that is doing practically no I/O, the disks will always blink every 10 seconds.)</p>
<p>The actual value of the watermark varies depending on the exact configuration of your environment: the Filer model, the Data ONTAP version, whether or not it’s HA, and whether SATA disks are present. Because SATA disks write more slowly than SAS disks, consistency points take longer to write to disk if they’re going to SATA disks. In order to combat this, Data ONTAP lowers the watermark in a system when SATA disks are present.</p>
<p>The size of NVRAM only really dictates a Filer’s performance envelope when doing a large amount of large sequential writes. But with the new FAS80xx family, NVRAM counts have grown massively — up to 4x of the FAS62xx family.</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2014/07/28/how-data-ontap-caches-assembles-and-writes-data/#respond" title="Comment on How Data ONTAP caches, assembles and writes data" sl-processed="1">Leave a comment</a>				</div>
	</div>
</div>
		
			
<div class="post-543 post type-post status-publish format-standard hentry category-performance category-storage" id="post-543">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2014/04/02/getting-started-with-netapp-storage-qos/" sl-processed="1">April 2, 2014</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2014/04/02/getting-started-with-netapp-storage-qos/" rel="bookmark" sl-processed="1">Getting started with NetApp Storage&nbsp;QoS</a></h3>		
		<div class="entry-content">
			<p>Storage QoS is a new feature in Clustered Data ONTAP 8.2. It is a full-featured QoS stack, which replaces the FlexShare stack of previous ONTAP versions. So what does it do and how does it work? Let’s take a look!</p>
<p>The administration of QoS involves two parts: policy groups, and policies. Policy <em>groups</em> define boundaries between workloads, and contain one or more storage objects. We can monitor, isolate and limit the workloads of storage objects from the biggest point of granularity down to the smallest — from entire Vservers, whole volumes, individual LUNs all the way down to single files. </p>
<p>The actual <em>policies</em> are behavior modifiers that are applied to a policy group. Right now, we can set throughput limits based on operation counts (i.e., IOps) or throughput counts (i.e., MB/s). When limiting throughput, storage QoS throttles traffic at the protocol stack. Therefore, a client whose I/O is being throttled will see queuing in their protocol stack (e.g., CIFS or NFS) and latency will eventually rise. However, the addition of this queuing will <strong>not</strong> affect the NetApp cluster’s resources.</p>
<p>In addition to the throttling of workloads, storage QoS also includes very effective measuring tools. And because QoS is “always on”, you don’t even need to have a policy group in order to monitor performance.</p>
<p>So, let’s get started. When it comes to creating our first policy, we actually require three steps in order for the policy to be applied to the workload:</p>
<ol>
<li>Create the policy (with <tt>qos policy-group create...</tt>)</li>
<li>Apply the policy to a Vserver (with <tt>vserver modify...</tt>)</li>
<li>Apply the policy to a volume (with <tt>vol modify...</tt>)</li>
<li>Monitor what’s going on (with <tt>qos statistics...</tt>)</li>
</ol>
<p>Before we start, let’s verify that we’re starting from a clean slate:</p>
<pre>dot82cm::&gt; qos policy-group show
This table is currently empty.</pre>
<p>Okay, good — no policy groups exist yet. Step one of three is to create the policy group itself, which we’ll call <tt>blog-group</tt>. In reality, you’d specify a throughput limit (either IOps or MB/s), but for now we won’t bother limiting the throughput:</p>
<pre>dot82cm::&gt; qos policy create blog-group -vserver vs0</pre>
<p>Let’s make sure the policy group was created:</p>
<pre>dot82cm::&gt; qos policy-group show                    
Name             Vserver     Class        Wklds Throughput  
---------------- ----------- ------------ ----- ------------
blog-group       vs0         user-defined -     0-INF</pre>
<p>Let’s confirm:</p>
<p>But, because we didn’t specify a throughput limit, the <strong>Throughput</strong> column is still showing 0 to infinity. Let’s add a limit of 1500 IOps:</p>
<pre>dot82cm::&gt; qos policy-group modify -policy-group blog-group -max-throughput 1500iops</pre>
<p>(If we wanted to limit that volume to 1500MB/s, we could have substituted <tt>1500mb</tt> for <tt>1500iops</tt>.)</p>
<p>And verify:</p>
<pre>dot82cm::&gt; qos policy-group show                                                
Name             Vserver     Class        Wklds Throughput  
---------------- ----------- ------------ ----- ------------
blog-group       vs0         user-defined 0     0-1500IOPS</pre>
<p>So, step three is to associate the new policy group with an actual object whose I/O we wish to throttle. The object can be one or many volumes, LUNs or files. For now though, we’ll apply it to a single volume, <tt>blog_volume</tt>:</p>
<pre>dot82cm::&gt; volume modify blog_volume -vserver vs0 -qos-policy-group blog-group

Volume modify successful on volume: blog_volume</pre>
<p>Let’s confirm that it was successfully modified:</p>
<pre>dot82cm::&gt; qos policy-group show                                                
Name             Vserver     Class        Wklds Throughput  
---------------- ----------- ------------ ----- ------------
blog-group       vs0         user-defined 1     0-1500IOPS</pre>
<p>Cool! We can see that <strong>Workloads</strong> has gone from 0 to 1. I’ve mounted that volume via NFS on a Linux VM, and will throw a bunch of workloads at it using <tt>dd</tt>. </p>
<p>While the workload is running, here’s how it looks:</p>
<pre>dot82cm::&gt; qos statistics workload characteristics show                         
Workload          ID     IOPS      Throughput      Request size    Read  Concurrency 
--------------- ------ -------- ---------------- --------------- ------- ----------- 
-total-              -      392         5.26MB/s          14071B     14%          14 
_USERSPACE_APPS     14      170       109.46KB/s            659B     32%           0 
_Scan_Backgro..  11702      115            0KB/s              0B      0%           0 
blog_volume-w..  11792     <strong>1679</strong>       104.94MB/s          65527B      0%           4</pre>
<p>As you can see, our volume <tt>blog_volume</tt> is pretty busy — it’s pushing almost 1,700 IOps at over 100MB/sec. So, let’s see if the throttling is effective. First, we’ll give the policy group a low throughput maximum:</p>
<pre>dot82cm::&gt; qos policy-group modify -policy-group blog-group -max-throughput 100iops</pre>
<p>Now let’s check its status:</p>
<pre>dot82cm::&gt; qos policy-group show
Name             Vserver     Class        Wklds Throughput  
---------------- ----------- ------------ ----- ------------
blog-group       vs0         user-defined 1     0-100IOPS</pre>
<p>Now let’s see how the Filer is doing:</p>
<pre>dot82cm::&gt; qos statistics workload characteristics show
Workload          ID     IOPS      Throughput      Request size    Read  Concurrency 
--------------- ------ -------- ---------------- --------------- ------- ----------- 
-total-              -      384         6.71MB/s          18333B     11%          33 
_USERSPACE_APPS     14      169         2.50MB/s          15528B     26%           0 
_Scan_Backgro..  11702      115            0KB/s              0B      0%           0 
blog_volume-w..  11792       <strong>83</strong>         5.19MB/s          65536B      0%          15 
-total-              -      207         4.81MB/s          24348B      0%          17</pre>
<p>You can see that the throughput has gone <strong>way</strong> down! In fact it’s gone below our limit of 80 IOps. And that, of course, is what’s supposed to happen. Now let’s remove the limit and see if things return to normal:</p>
<pre>dot82cm::&gt; qos policy-group modify -policy-group blog-group -max-throughput none

dot82cm::&gt; qos statistics workload characteristics show                            
Workload          ID     IOPS      Throughput      Request size    Read  Concurrency 
--------------- ------ -------- ---------------- --------------- ------- ----------- 
-total-              -     1073        44.37MB/s          43363B      8%           1 
blog_volume-w..  11792      <strong>626</strong>        39.12MB/s          65492B      0%           1 
_USERSPACE_APPS     14      302         4.90MB/s          17041B     29%           0 
_Scan_Backgro..  11702      115            0KB/s              0B      0%           0 
-total-              -      263       471.86KB/s           1837B     19%           0</pre>
<p>Because we can apply the QoS policy groups to entire Vservers, volumes, files &amp; LUNs, it is important to keep track of what’s applied where. This is how you’d apply a policy group to an individual volume:</p>
<pre>dot82cm::&gt; volume modify -vserver vs0 -volume blog_volume -qos-policy-group blog-group</pre>
<p>(To remove the policy, set the <tt>-qos-policy-group</tt> field to <tt>none</tt>.)</p>
<p>To apply a policy group against an <strong>entire Vserver</strong> (in this case, Vserver <tt>vs0</tt>):</p>
<pre>dot82cm::&gt; vserver modify -vserver vs0 -qos-policy-group blog-group</pre>
<p>(Again, to remove the policy, set the <tt>-qos-policy-group</tt> field to <tt>none</tt>.)</p>
<p>To see which volumes are assigned our policy group:</p>
<pre>dot82cm::&gt; volume show -vserver vs0 -qos-policy-group blog-group                       
Vserver   Volume       Aggregate    State      Type       Size  Available Used%
--------- ------------ ------------ ---------- ---- ---------- ---------- -----
vs0       blog_volume  aggr1_node1  online     RW          1GB    365.8MB   64%</pre>
<p>To see <strong>all</strong> volumes in <strong>all</strong> Vservers’ QoS policy groups:</p>
<pre>dot82cm::&gt; volume show -vserver * -qos-policy-group * -fields vserver,volume,qos-policy-group

vserver    volume qos-policy-group 
---------- ------ ---------------- 
vs0        blog_empty 
                  -                
vs0        blog_volume 
                  blog-group</pre>
<p>To see a Vserver’s full configuration, including its QoS policy group:</p>
<pre>dot82cm::&gt; vserver show -vserver vs0

                                    Vserver: vs0
                               Vserver Type: data
                               Vserver UUID: c280658e-bd77-11e2-a567-123478563412
                                Root Volume: vs0_root
                                  Aggregate: aggr1_node2
                        Name Service Switch: file, nis, ldap
                        Name Mapping Switch: file, ldap
                                 NIS Domain: <span class="skimlinks-unlinked">newengland.netapp.com</span>
                 Root Volume Security Style: unix
                                LDAP Client: -
               Default Volume Language Code: C
                            Snapshot Policy: default
                                    Comment: 
                 Antivirus On-Access Policy: default
                               Quota Policy: default
                List of Aggregates Assigned: -
 Limit on Maximum Number of Volumes allowed: unlimited
                        Vserver Admin State: running
                          Allowed Protocols: nfs, cifs, ndmp
                       Disallowed Protocols: fcp, iscsi
            Is Vserver with Infinite Volume: false
                           <strong>QoS Policy Group: -</strong></pre>
<p>To get a list of all Vservers by their policy groups:</p>
<pre>dot82cm::&gt; vserver show -vserver * -qos-policy-group * -fields vserver,qos-policy-group 
vserver qos-policy-group 
------- ---------------- 
dot82cm -                
dot82cm-01 
        -                
dot82cm-02 
        -                
vs0     blog-group       
4 entries were displayed.</pre>
<p>If you’re in a hurry and want to remove all instances of a policy <em>from volumes</em> in a particular Vserver:</p>
<pre>dot82cm::&gt; vol modify -vserver vs0 -volume * -qos-policy-group none</pre>
<p>That should be enough to get us going. Stay tuned, because in the next episode I’ll show some video with iometer running!</p>
<p>Documentation:</p>
<ul>
<li><a href="https://library.netapp.com/ecm/ecm_download_file/ECMP1196798" sl-processed="1">Clustered Data ONTAP® 8.2 System Administration Guide for Cluster Administrators</a></li>
</ul>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2014/04/02/getting-started-with-netapp-storage-qos/#respond" title="Comment on Getting started with NetApp Storage QoS" sl-processed="1">Leave a comment</a>				</div>
	</div>
</div>
		
			
<div class="post-428 post type-post status-publish format-standard hentry category-performance category-storage" id="post-428">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2013/07/30/differences-between-netapp-flash-cache-and-flash-pool/" sl-processed="1">July 30, 2013</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2013/07/30/differences-between-netapp-flash-cache-and-flash-pool/" rel="bookmark" sl-processed="1">Differences between NetApp Flash Cache and Flash&nbsp;Pool</a></h3>		
		<div class="entry-content">
			<p>NetApp’s Flash Cache product has been around for several years, the Flash Pool product is much newer. While there is a lot of overlap between the two products, there are distinct differences as well. The goal of both technologies is the same: to accelerate reads by serving data from solid-state memory instead of spinning disk. Although they produce the same end result, they are built differently and they operate differently.</p>
<p>First of all, why do we bother with read (and write caching)? Because there’s a <strong>significant</strong> speed differential depending on the source form which we serve data. RAM is much faster than SSD, and SSD is much faster than rotating disk. This graphic illustrates the paradigm:</p>
<p><a href="https://bitpushr.files.wordpress.com/2013/07/why-cache.jpg" sl-processed="1"><img class="alignnone size-full wp-image-452" alt="why-cache" src="./Performance   Bitpushr's Blog_files/why-cache.jpg" width="500" height="323"></a></p>
<p>(Note that we can optimize the performance of 100%-rotating disk environments with techniques like read-aheads, buffering and the like.) </p>
<p><em>Sequential</em> reads can be read off rotating disks extremely quickly, and very rarely needs to be accelerated by solid-state storage. <em>Random</em> reads are much better served from Flash than rotating disks, and <em>very recent random</em> reads can be served from buffer cache RAM (i.e., the controller’s RAM) — which is an order of magnitude faster again. Although Flash Pool and Flash Cache provide a caching mechanism for those random reads (and some other I/O operations), they do so in different ways.</p>
<p>Here is a quick overview of some of the details that come up when comparing the two technologies:</p>
<table>
<tbody>
<tr>
<td><b>Detail</b></td>
<td><b>Flash Cache</b></td>
<td><b>Flash Pool</b></td>
</tr>
<tr>
<td>Physical entity</td>
<td>PCI-e card</td>
<td>SSD drive</td>
</tr>
<tr>
<td>Physical location</td>
<td>Controller head</td>
<td>Disk shelf</td>
</tr>
<tr>
<td>Logical location</td>
<td>Controller bus</td>
<td>Disk stack/loop</td>
</tr>
<tr>
<td>Logical accessibility</td>
<td>Controller head</td>
<td>Disk stack/loop</td>
</tr>
<tr>
<td>Cache mechanism</td>
<td>First-in, first-out</td>
<td>Temperature map</td>
</tr>
<tr>
<td>Cache persistence on failover</td>
<td>Requires re-warm</td>
<td>Yes</td>

</tr><tr>
<td>Cache data support</td>
<td>Reads</td>
<td>Reads, random overwrites</td>
</tr>
<tr>
<td>Cache metadata support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Cache sequential data support</td>
<td>Yes, with <tt>lopri</tt> mode</td>
<td>No</td>
</tr>
<tr>
<td>Infinite Volume support</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Management granularity</td>
<td>System-level for all aggregates &amp; volumes</td>
<td>Aggregate-level with per-volume policies</td>
</tr>
<tr>
<td>32-bit aggregate support</td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>64-bit aggregate support</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>RAID protection</td>
<td>N/A</td>
<td>RAID-4, RAID-DP (recommended)</td>
</tr>
<tr>
<td>Minimum quantity</td>
<td>1 PCI-e card</td>
<td>3 SSD drives</td>
</tr>
<tr>
<td>Minimum Data ONTAP version</td>
<td>7.3.2</td>
<td>8.1.1</td>
</tr>
<tr>
<td>Removable</td>
<td>Yes</td>
<td>Yes, but aggregate must be destroyed</td>
</tr>
</tbody>
</table>
<p>The most basic difference between the two technologies is that Flash Cache is a PCI-e card (or cards) that sits in the NetApp controller, whereas Flash Pool are SSD drives that sits in NetApp shelves. This gives way to some important points. First off, Flash Cache memory that is accessed via the PCI-e bus is always going to have a much higher potential for throughput than Flash Pool memory that sits on a SAS stack. Secondly, the two different architectures result in different means of logical accessibility — for Flash Cache, <strong>any and all aggregates that sit on a controller with a Flash Cache card</strong> can be accelerated. With Flash Pool, <strong>any and all aggregates that sit in a disk stack (or loop) with a Flash Pool SSD</strong> can be accelerated. This gives way to the fact that in the event of planned downtime (such as a takeover/giveback), the Flash Cache card’s cache data can be copied to the HA pair’s partner node (“rewarming”), but in the event of <em>unplanned</em> downtime (such as a panic), the Flash Cache card’s cache data will be lost. The corollary of this is that if a controller with Flash Pool fails, the aggregate will fail over to the other controller <em>and the Flash Pool will be accessible on the other controller</em>. </p>
<p>Another important difference is Flash Pool’s ability to cache <strong>random overwrites</strong>. First, let’s define what we’re talking about: a random overwrite is a small, random write of a block(s) that was recently written to HDD, and now we’re seeing a request for that block to now be <em>over</em>written. The fact that we’re talking only about random overwrites is important, because Flash Pools do not accelerate traditional and sequential writes — Data ONTAP is already optimized for writes. Rather, Flash Pool lets us cache these random overwrites by allowing the Consistency Points (CPs) that contain random overwrites to be written into the SSD cache. Writing to SSD is up to 2x quicker than writing to rotating disk, which means that the Consistency Point occurs faster. Random overwrites are the most expensive (i.e., slowest) operation that we can perform to a rotating disk, so it’s in our interest to accelerate them.</p>
<p>Although Flash Cache cannot cache random overwrites, it <em>can</em> function as a <strong>write-through cache</strong> via the use of the <tt>lopri</tt> mode. In some workloads, we may desire that recently-written data be immediately read just after being written (the so-called “read after write” scenario). For these workloads, Flash Cache can improve performance by caching <em>recently-written</em> blocks rather than having to seek the rotating disks for that recently-written data. Note that we are <em>not</em> writing the client data into Flash Cache primarily, but rather we are writing the data to rotating disk <em>through</em> the Flash Cache — thus, a write-through cache. Flash Cache serves as a write-through cache under two scenarios. Flash Cache will serve as a write-through cache if the Flash Cache is less than 70% full if <tt>lopri</tt> is disabled. If <tt>lopri</tt> is enabled, it will always serve as a write-through cache. The enabling of <tt>lopri</tt> mode in Flash Cache will also cache sequential reads, whereas Flash Pool has no mechanism for caching sequential data.</p>
<p>Continuing this exercise are the differences between <strong>how</strong> Flash Cache and Flash Pools actually cache data. Flash Cache is populated with blocks of data that are being evicted from the controller’s primary memory (i.e. its RAM) but are requested by client I/O. Flash Cache utilizes a <strong>first-in, first-out</strong> algorithm: when a new block of data arrives in the Flash Cache, an existing block of data is purged. When the next new block arrives, our previous block is now one step closer to its own eviction. (The number of blocks that can exist in cache depends on the size of your Flash Cache card.)</p>
<p>Flash Pool is initially populated the same way: when blocks of data match the cache insertion policy and have been evicted from the controller’s primary memory but are requested by client I/O. Flash Pool utilizes a <strong>temperature map</strong> algorithm: when a new block of data arrives in the Flash Pool, it is assigned a neutral temperature. Data ONTAP keeps track of the temperature of blocks by forming a <strong>heat map</strong> of those blocks.</p>
<p>This is what a temperature map looks like, and shows how a <strong>read</strong> gets evicted:</p>
<p><a href="https://bitpushr.files.wordpress.com/2013/07/read-cache-mgmt.jpg" sl-processed="1"><img src="./Performance   Bitpushr's Blog_files/read-cache-mgmt.jpg" alt="read-cache-mgmt" width="500" height="185" class="alignnone size-full wp-image-472"></a></p>
<p>You can see the read cache process: a block gets inserted and labeled with a neutral temperature. If the block gets accessed by clients, the scanner sees this and <strong>increases</strong> the temperature of the block — meaning it will stay in cache for longer. If the block <em>doesn’t</em> get accessed, the scanner sees this and <strong>decreases</strong> the temperature of the block. When it gets cold enough, it gets evicted. This means that we can keep hot data while discarding cold data. Note that the eviction scanner only starts running when the cache is at least 75% utilized.</p>
<p>This is how a<strong> random overwrite</strong> gets evicted:</p>
<p><a href="https://bitpushr.files.wordpress.com/2013/07/write-cache-mgmt.jpg" sl-processed="1"><img src="./Performance   Bitpushr's Blog_files/write-cache-mgmt.jpg" alt="write-cache-mgmt" width="500" height="186" class="alignnone size-full wp-image-474"></a></p>
<p>The hotter the block, the farther right on the temperature gauge and the longer the block is kept in cache.&nbsp;Again, you can see the process: a block gets inserted and labeled with a neutral temperature. If the block gets randomly overwritten again by clients, the block is re-inserted and labeled with a neutral temperature. Unlike a read, a random overwrite <em>cannot have its temperature increased</em> — it can only be re-inserted.</p>
<p>Note that I mentioned Flash Pool actually has RAID protection. This is to ensure the integrity of the cached data on disk, but it’s important to remember that read cache data is just that – a cache of read data. <strong>The permanent copy of the data always lives on rotating disks</strong>. If your Flash Pool SSD drives all caught fire at the same time, the read cache would be disabled but you would <u>not</u> lose data.</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2013/07/30/differences-between-netapp-flash-cache-and-flash-pool/#comments" title="Comment on Differences between NetApp Flash Cache and Flash Pool" sl-processed="1">4 Comments</a>				</div>
	</div>
</div>
		
			
<div class="post-246 post type-post status-publish format-standard hentry category-performance category-storage category-uncategorized" id="post-246">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2011/11/29/why-cache-is-better-than-tiering-for-performance/" sl-processed="1">November 29, 2011</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2011/11/29/why-cache-is-better-than-tiering-for-performance/" rel="bookmark" sl-processed="1">Why cache is better than tiering for&nbsp;performance</a></h3>		
		<div class="entry-content">
			<p>When it comes to adjusting (either increasing or decreasing) storage performance, two approaches are common: caching and tiering. Caching refers to a process whereby commonly-accessed data gets <em>copied</em> into the storage controller’s high-speed, solid-state cache. Therefore, a client request for cached data never needs to hit the storage’s disk arrays at all; it is simply served right out of the cache. As you can imagine this is very, very fast.</p>
<p>Tiering, in contrast, refers to the <em>movement</em> of a data set from one set of disk media to another; be it from slower to faster disks (for high-performance, high-importance data) or from faster to slower disks (for low-performance, low-importance data). For example, you may have your high-performance data living on a SSD, FC or SAS disk array, and your low-performance data may only require the IOps that can be provided by relatively low-performance SATA disks.</p>
<p>Both solutions have pros and cons. Cache is typically less configurable by the user, as the cache’s operation will be managed by the storage controller. It is considerably faster, as the cache will live on the bus — it won’t need to traverse the disk subsystem (via SAS, FCAL etc.) to get there, nor will it have to compete with other I/O along the disk subsystem(s). But, it’s also more expensive: high-grade, high-performance solid-state cache memory is more costly than SSD disks. Last but not least, the cache needs to “warm up” in order to be effective — though in the real world this does not take long at all!</p>
<p>Tiering’s main advantages are that it is more easily tunable by the customer. However, all is not simple: in complex environments, tuning the tiering may literally be too complex to bother with. Also, manual tiering relies on you being able to predict the needs of your storage, and adjust tiering automatically: how do you know tomorrow which business application will require the hardest hit? Again, in complex environments, this relatively simply question may be decidedly difficult to answer. On the positive side, tiering offers more flexibility in terms of <em>where</em> you put your data. Cache is cache, regardless of environment; data is either on the cache or it’s not. On the other hand, tiering lets you take advantage of more types of storage: SSD or FC or SAS or SATA, depending on your business needs. </p>
<p>But if you’re tiering for performance (which is the focus of this blog post), then you have to deal with one big issue: the very act of tiering increases the load on your storage system! Tiering actually <strong>creates</strong> latency as it occurs: in order to move the data from one storage tier to another, we are literally creating IOps on the storage back-end in order to accomplish the performance increase! That is, in order to get higher performance, we’re actually hurting performance in the meantime (i.e., while the data is moving around.)</p>
<p>In stark contrast, caching reduces latency <em>and</em> increases throughput as it happens. This is because the data doesn’t really move: the first time data is requested, a cache request is made (and misses — it’s not in the cache yet) and the data is served from disk. On it’s way to the customer, though, the data will stay in the cache for a while. If it’s requested again, another cache request is made (and hits — the data is already in the cache) and the data is served from cache. And it’s served fast!</p>
<p>(It’s worthwhile to note that NetApp’s cache solutions actually offer more than “simple” caching: we can even cache things like file/block metadata. And customers can tune their cache to behave how they want it.)</p>
<p>Below is a graph from a customer’s benchmark. It was a large SQL Server read, but what is particularly interesting is the behavior of the of the graph: throughput (in red) goes up while latency (in blue) actually drops!</p>
<p><a href="https://bitpushr.files.wordpress.com/2011/11/flashcache.png" sl-processed="1"><img src="./Performance   Bitpushr's Blog_files/flashcache.png" alt="" title="flashcache" width="300" height="189" class="alignnone size-medium wp-image-248"></a></p>
<p>If you were seeking a performance augmentation via tiering, there would have been two different possibilities. If your data was already tiered, throughput will go up while latency will remain the same. If your data wasn’t already tiered, throughput will decrease as latencies will increase as the data is tiered; only after the tiering is completed will you actually see an increase in throughput.</p>
<p>For gaining performance in your storage system, caching is simply better than tiering.</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2011/11/29/why-cache-is-better-than-tiering-for-performance/#comments" title="Comment on Why cache is better than tiering for performance" sl-processed="1">2 Comments</a>				</div>
	</div>
</div>
		
			
<div class="post-185 post type-post status-publish format-standard hentry category-performance category-storage" id="post-185">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2011/08/28/blackboard-oracle-benchmarks-on-ssd-storage/" sl-processed="1">August 28, 2011</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2011/08/28/blackboard-oracle-benchmarks-on-ssd-storage/" rel="bookmark" sl-processed="1">BlackBoard &amp; Oracle benchmarks on SSD&nbsp;storage</a></h3>		
		<div class="entry-content">
			<p>I spent Friday benchmarking two platforms against each other: our production BlackBoard instance (9.1 SP6) versus a development BlackBoard instance (also 9.1 SP6) that I spun up for this test. The unique thing about this test was that I put the development instance entirely on SSD storage.</p>
<p>Production instance:</p>
<ul>
<li>BlackBoard 9.1 SP6 on VMware ESXi VM (Linux)
</li><li>Oracle 10g R2 on HP BL465c G6 Blade (Linux)
</li><li>BlackBoard on NetApp filer; 1 volume on a 45-disk RAID-DP aggregate
</li><li>Oracle on local HP SAS RAID-1 disk set
</li></ul>
<p>Development instance: </p>
<ul>
<li>BlackBoard 9.1 SP6 on VMware ESXi VM (Linux)
</li><li>Oracle 10g R2 on VMware ESXi VM (Linux)
</li><li>BlackBoard on FusionIO ioDrive SSD
</li><li>Oracle also on (the same) FusionIO ioDrive SSD
</li></ul>
<p>With the exception of workloads (itself a significant difference, to be sure), I tried to keep everything else the same — same patches, same OS revision, same LDAP &amp; SSL configurations, etc.</p>
<p>To test performance, I had a suggestion from Steve (<a href="http://twitter.com/#/Seven_Seconds" sl-processed="1">@Seven_Seconds</a>) at BlackBoard: hit the <tt>/webapps/login/</tt> page with <a href="http://httpd.apache.org/docs/2.0/programs/ab.html" sl-processed="1">ab</a>, the Apache benchmark tool.</p>
<p>The results were, well, strange. With each test of <strong>em</strong> running 5000 requests to <tt>/webapps/login/</tt>, here is a graph:</p>
<p><img src="./Performance   Bitpushr's Blog_files/chart.jpg" alt="NetApp v SSD"></p>
<p>You can see that the <strong>mean</strong> response times SSD (blue line) equals or out-performs the NetApp (red line) at every concurrency value tested. What is particularly interesting, though, is the incredibly slow rate at which the <strong>standard deviation</strong> of the SSD (purple bars) grows; compare that with the rate at which the standard deviation of the response times of the NetApp (green bars) grow.</p>
<p>Right now, I don’t have a clear answer as to why the deviations from the mean grow so differently. The NetApp filer has a significantly different workload to the SSD (the SSD has no workload other than this test), but the exponential growth of the NetApp’s standard deviation is something I’ll need to investigate further.</p>
<p>Last but not least, there were no SSD-specific tuning options put in place, nor where there any NetApp-specific tuning options. This was as vanilla as both installs could get in order to keep everything relatively comparable.</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2011/08/28/blackboard-oracle-benchmarks-on-ssd-storage/#comments" title="Comment on BlackBoard &amp; Oracle benchmarks on SSD storage" sl-processed="1">1 Comment</a>				</div>
	</div>
</div>
		
			
<div class="post-143 post type-post status-publish format-standard hentry category-performance" id="post-143">
			<h5 class="post-date"><abbr class="published">
				<a href="https://bitpushr.wordpress.com/2011/08/04/performance-in-strange-places/" sl-processed="1">August 4, 2011</a></abbr></h5>
		<div class="post-content">
		<h3 class="entry-title"><a href="https://bitpushr.wordpress.com/2011/08/04/performance-in-strange-places/" rel="bookmark" sl-processed="1">Performance in strange&nbsp;places</a></h3>		
		<div class="entry-content">
			<p>Sometimes, you find performance improvements in places where you weren’t looking for an improvement. I will give an example: at a meeting last week, our webmaster was passing on complaints from users that their Content Management System was performing poorly. He had done a bunch of investigative work and concluded that the problem was likely an Active Directory problem. While that’s not exactly what he <em>meant</em>, it’s what he said, and our Active Directory administrator was a little annoyed.</p>
<p>What the Webmaster actually meant was that, when publishing several thousand files that live on a NetApp filer that is Active Directory-aware, there may be issues if you’re performing thousands of LDAP lookups serially. Which is a valid point: that can get computationally expensive. So, we got to talking about it, and another engineer mentioned <strong>nscd</strong>, the <a href="http://linux.die.net/man/8/nscd" target="_blank" sl-processed="1">name services caching daemon</a> — something I didn’t know existed outside NIS (i.e., legacy platforms).</p>
<p>Long story short, it made for an amusing (and satisfying) test. </p>
<pre>/home#  time ls -al | wc -l
3965

real    <strong>0m19.547s</strong>
user    0m3.729s
sys     0m1.700s</pre>
<p>Almost 20 seconds to perform a lookup on an NFS-mounted directory that has almost 4,000 unique Active Directory users. That’s a lot of UIDs and GIDs! Now, we install nscd, and run the same test once to populate the cache.</p>
<p>Then we run the test again, with a populated cache. Care to see the results?</p>
<pre>/home# time ls -al | wc -l     
3965

real    <strong>0m2.241s</strong>
user    0m0.360s
sys     0m0.570s</pre>
<p>From 19 seconds down to less than 3 seconds. Quite the improvement! Obviously you may need to adjust your cache parameters, particularly if this is a fairly inactive system most of the time but does have the occasional need for performance — in such a case, you may set your cache to expire unusually slowly. The lesson from this, though, is that sometimes you find gains when you weren’t looking. Or even in places you didn’t know gains could be found.</p>
					</div>
	</div>
	<div class="post-meta">
		<div class="comments">
					<a href="https://bitpushr.wordpress.com/2011/08/04/performance-in-strange-places/#respond" title="Comment on Performance in strange places" sl-processed="1">Leave a comment</a>				</div>
	</div>
</div>
		
	<div class="navigation">
		<div class="prev"><a href="https://bitpushr.wordpress.com/category/performance/page/2/" sl-processed="1">&lt; Older Posts</a></div>
		<div class="next"></div>
	</div>

	
	</div><!-- #core-content -->


</div><!-- #site-wrapper -->

<div id="footer">

	

	<!-- Search Field -->
	<div class="footer-content">
		<form method="get" id="searchform" action="https://bitpushr.wordpress.com/">
			<div id="search">
				<input type="text" value="" name="s" id="s">
				<input type="submit" id="searchsubmit" value="Search">
			</div>
		</form>
		<p>
			<a href="https://wordpress.com/?ref=footer_website" sl-processed="1">Create a free website or blog at WordPress.com</a>.
			<a href="https://wordpress.com/themes/manifest/" title="Learn more about this theme" sl-processed="1">The Manifest Theme</a>.		</p>
	</div>
</div><!-- #footer -->

		<script type="text/javascript">
		//<![CDATA[
		var infiniteScroll = {"settings":{"id":"core-content","ajaxurl":"https:\/\/bitpushr.wordpress.com\/?infinity=scrolling","type":"scroll","wrapper":true,"wrapper_class":"infinite-wrap","footer":true,"click_handle":"1","text":"Older posts","totop":"Scroll back to top","currentday":"04.08.11","order":"DESC","scripts":[],"styles":[],"google_analytics":false,"offset":0,"history":{"host":"bitpushr.wordpress.com","path":"\/category\/performance\/page\/%d\/","use_trailing_slashes":true,"parameters":""},"query_args":{"category_name":"performance","error":"","m":"","p":0,"post_parent":"","subpost":"","subpost_id":"","attachment":"","attachment_id":0,"name":"","static":"","pagename":"","page_id":0,"second":"","minute":"","hour":"","day":0,"monthnum":0,"year":0,"w":0,"tag":"","cat":1930,"tag_id":"","author":"","author_name":"","feed":"","tb":"","paged":0,"comments_popup":"","meta_key":"","meta_value":"","preview":"","s":"","sentence":"","fields":"","menu_order":"","category__in":[],"category__not_in":[],"category__and":[],"post__in":[],"post__not_in":[],"tag__in":[],"tag__not_in":[],"tag__and":[],"tag_slug__in":[],"tag_slug__and":[],"post_parent__in":[],"post_parent__not_in":[],"author__in":[],"author__not_in":[],"posts_per_page":7,"ignore_sticky_posts":false,"suppress_filters":false,"cache_results":false,"update_post_term_cache":true,"update_post_meta_cache":true,"post_type":"","nopaging":false,"comments_per_page":"50","no_found_rows":false,"order":"DESC"},"last_post_date":"2011-08-04 15:35:52","stats":"blog=8899863&v=wpcom&tz=0&user_id=0&subd=bitpushr&x_pagetype=infinite"}};
		//]]>
		</script>
		<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/gprofiles.js"></script>
<script type="text/javascript">
/* <![CDATA[ */
var WPGroHo = {"my_hash":""};
/* ]]> */
</script>
<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/wpgroho.js"></script>

	<script>
		//initialize and attach hovercards to all gravatars
		jQuery( document ).ready( function( $ ) {
			if ( typeof Gravatar.init !== "function" ) {
				return;
			}			

			Gravatar.profile_cb = function( hash, id ) {
				WPGroHo.syncProfileData( hash, id );
			};
			Gravatar.my_hash = WPGroHo.my_hash;
			Gravatar.init( 'body', '#wp-admin-bar-my-account' );
		});
	</script>

		<div style="display:none">
	</div>
		<div id="infinite-footer" style="bottom: -50px;">
			<div class="container">
				<div class="blog-info">
					<a id="infinity-blog-title" href="https://bitpushr.wordpress.com/" rel="home" title="Scroll back to top" sl-processed="1">
						Bitpushr's Blog					</a>
				</div>
				<div class="blog-credits">
					<a href="https://wordpress.com/?ref=footer_blog" sl-processed="1">Blog at WordPress.com</a>. <a href="https://wordpress.com/themes/manifest/" title="Learn more about this theme" sl-processed="1">The Manifest Theme</a>.				</div>
			</div>
		</div><!-- #infinite-footer -->
		
	<div id="bit" class="loggedout-follow-normal" style="bottom: -252px;">
		<a class="bsub" href="javascript:void(0)" sl-processed="1"><span id="bsub-text">Follow</span></a>
		<div id="bitsubscribe">

					<h3><label for="loggedout-follow-field">Follow “Bitpushr's Blog”</label></h3>

			<form action="https://subscribe.wordpress.com/" method="post" accept-charset="utf-8" id="loggedout-follow">
			<p>Get every new post delivered to your Inbox.</p>

			<p id="loggedout-follow-error" style="display: none;"></p>

						<p class="bit-follow-count">Join 189 other followers</p>
			<p><input type="email" name="email" value="Enter your email address" onfocus="this.value=(this.value==&quot;Enter your email address&quot;) ? &quot;&quot; : this.value;" onblur="this.value=(this.value==&quot;&quot;) ? &quot;Enter email address&quot; : this.value;" id="loggedout-follow-field"></p>

			<input type="hidden" name="action" value="subscribe">
			<input type="hidden" name="blog_id" value="8899863">
			<input type="hidden" name="source" value="https://bitpushr.wordpress.com/category/performance/">
			<input type="hidden" name="sub-type" value="loggedout-follow">

			<input type="hidden" id="_wpnonce" name="_wpnonce" value="673aa2dd44"><input type="hidden" name="_wp_http_referer" value="/category/performance/">
			<p id="bsub-subscribe-button"><input type="submit" value="Sign me up"></p>
			</form>
					<div id="bsub-credit"><a href="https://wordpress.com/?ref=lof" sl-processed="1">Build a website with WordPress.com</a></div>
		</div><!-- #bitsubscribe -->
	</div><!-- #bit -->

	<div id="carousel-reblog-box">
		<form action="" name="carousel-reblog">
			<textarea id="carousel-reblog-content" name="carousel-reblog-content" onclick="if ( this.value == &#39;Add your thoughts here... (optional)&#39; ) { this.value = &#39;&#39;; }" onblur="if ( this.value == &#39;&#39; || this.value == &#39;&#39; ) { this.value = &#39;Add your thoughts here... (optional)&#39;; }">Add your thoughts here... (optional)</textarea>
			<label for="carousel-reblog-to-blog-id" id="carousel-reblog-lblogid">Post to</label>
			<select name="carousel-reblog-to-blog-id" id="carousel-reblog-to-blog-id">
						</select>

			<div class="submit">
				<span class="canceltext"><a href="" class="cancel" sl-processed="1">Cancel</a></span>
				<input type="submit" name="carousel-reblog-submit" class="button" id="carousel-reblog-submit" value="Reblog Post">
				<input type="hidden" id="carousel-reblog-blog-id" value="8899863">
				<input type="hidden" id="carousel-reblog-blog-url" value="https://bitpushr.wordpress.com">
				<input type="hidden" id="carousel-reblog-blog-title" value="Bitpushr&#39;s Blog">
				<input type="hidden" id="carousel-reblog-post-url" value="">
				<input type="hidden" id="carousel-reblog-post-title" value="">
			</div>

			<input type="hidden" id="_wpnonce" name="_wpnonce" value="f1d92853a0"><input type="hidden" name="_wp_http_referer" value="/category/performance/">		</form>

		<div class="arrow"></div>
	</div>
<link rel="stylesheet" id="all-css-0" href="./Performance   Bitpushr's Blog_files/jetpack-carousel.css" type="text/css" media="all">
<!--[if lte IE 8]>
<link rel='stylesheet' id='jetpack-carousel-ie8fix-css'  href='https://s1.wp.com/wp-content/mu-plugins/carousel/jetpack-carousel-ie8fix.css?m=1412618825g&#038;ver=20121024' type='text/css' media='all' />
<![endif]-->
<link rel="stylesheet" id="all-css-2" href="./Performance   Bitpushr's Blog_files/tiled-gallery.css" type="text/css" media="all">
<script type="text/javascript">
/* <![CDATA[ */
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/bitpushr.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"f829155644","display_exif":"1","display_geo":"1","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/bitpushr.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fbitpushr.wordpress.com%2F2011%2F08%2F04%2Fperformance-in-strange-places%2F","local_comments_commenting_as":"<fieldset><label for=\"email\">Email (Required)<\/label> <input type=\"text\" name=\"email\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-email-field\" \/><\/fieldset><fieldset><label for=\"author\">Name (Required)<\/label> <input type=\"text\" name=\"author\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-author-field\" \/><\/fieldset><fieldset><label for=\"url\">Website<\/label> <input type=\"text\" name=\"url\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-url-field\" \/><\/fieldset>","reblog":"Reblog","reblogged":"Reblogged","reblog_add_thoughts":"Add your thoughts here... (optional)","reblogging":"Reblogging...","post_reblog":"Post Reblog","stats_query_args":"blog=8899863&v=wpcom&tz=0&user_id=0&subd=bitpushr","is_public":"1"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/bitpushr.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"f829155644","display_exif":"1","display_geo":"1","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/bitpushr.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fbitpushr.wordpress.com%2F2011%2F08%2F04%2Fperformance-in-strange-places%2F","local_comments_commenting_as":"<fieldset><label for=\"email\">Email (Required)<\/label> <input type=\"text\" name=\"email\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-email-field\" \/><\/fieldset><fieldset><label for=\"author\">Name (Required)<\/label> <input type=\"text\" name=\"author\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-author-field\" \/><\/fieldset><fieldset><label for=\"url\">Website<\/label> <input type=\"text\" name=\"url\" class=\"jp-carousel-comment-form-field jp-carousel-comment-form-text-field\" id=\"jp-carousel-comment-form-url-field\" \/><\/fieldset>","reblog":"Reblog","reblogged":"Reblogged","reblog_add_thoughts":"Add your thoughts here... (optional)","reblogging":"Reblogging...","post_reblog":"Post Reblog","stats_query_args":"blog=8899863&v=wpcom&tz=0&user_id=0&subd=bitpushr","is_public":"1"};
/* ]]> */
</script>
<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/saved_resource(4)"></script>
<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/widgets.js"></script>
<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/saved_resource(5)"></script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script>	<script type="text/javascript">
	var skimlinks_pub_id = "725X584219"
	var skimlinks_sitename = "bitpushr.wordpress.com";
	</script>
	<script type="text/javascript" src="./Performance   Bitpushr's Blog_files/725X1342.skimlinks.js"></script><script type="text/javascript">
			jQuery.extend( infiniteScroll.settings.scripts, ["jquery","mobile-useragent-info","jquery-core","jquery-migrate","postmessage","jquery_inview","jetpack_resize","loggedout-subscribe","spin","jquery.spin","grofiles-cards","wpgroho","devicepx","the-neverending-homepage","swfobject","videopress","jetpack-carousel","twitter-widgets","twitter-widgets-infinity","twitter-widgets-pending","tiled-gallery"] );
			jQuery.extend( infiniteScroll.settings.styles, ["smileyproject","jetpack_likes","loggedout-subscribe","the-neverending-homepage","infinity-manifest","wpcom-core-compat-playlist-styles","mp6hacks","manifest","noticons","geo-location-flair","reblogging","a8c-global-print","h4-global","manifest-ie","jetpack-carousel","tiled-gallery","jetpack-carousel-ie8fix"] );
		</script><script src="./Performance   Bitpushr's Blog_files/w.js" type="text/javascript"></script>
<script type="text/javascript">
st_go({'blog':'8899863','v':'wpcom','tz':'0','user_id':'0','subd':'bitpushr'});
ex_go({'crypt':'UE40eW5QN0p8M2Y/RE1LVmwrVi5vQS5fVFtfdHBbPyw1VXIrU3hWLHhzVndTdktBX0ddJnpXRjVaOTd6fj1YMX4ydzR6MmRCYnxkNmclM11PdjU9JS80NWlURCVDWHdySjh8XVMzP1V+ZlNZQS9uUHRpTSZuODFUMF1BLSxiU0ZYdjNsX2ptfl1nSX5UP2lCQXJLPWNZP1toPT9ndVotXWNwbXlWZ21XMGJTR1VbWGQlJUk2OWE/dVEsTGJNSW13MUt0U089Kz8lfmpfU2E4ZHNjbE5pfmNTWEU2ODdQfDVFSnZyTFNPLDNKbG0xX2NIWHRLQ3BYZSxabGF1ZUJYUlNBdWYwaj8/ZVFa'});
addLoadEvent(function(){linktracker_init('8899863',0);});
	</script><img id="wpstats" src="./Performance   Bitpushr's Blog_files/g.gif" alt=""><img id="wpstats2" src="./Performance   Bitpushr's Blog_files/g(1).gif" alt="" style="display:none">
<noscript>&lt;img src="https://pixel.wp.com/b.gif?v=noscript" style="height:0px;width:0px;overflow:hidden" alt="" /&gt;</noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>

</body></html>